---
layout: post
title:  "Bragi - Telemetry and Mobile App Command and Control"
author: "Patric Boscolo, Oliver Scheer"
author-link: "https://twitter.com/patricsmsdn"
#author-image: "{{ site.baseurl }}/images/authors/photo.jpg"
date:   2016-05-19
categories: [IoT, PowerBI]
color: "blue"
#image: "{{ site.baseurl }}/images/imagename.png" #should be ~350px tall
excerpt: In this paper you will learn how we build telemetry service based on microsoft azure iot hub and visualize the data in PowerBi.
language: [English]
verticals: [Communications]
geolocation: [Europe]
published: false
---

**Solution overview**
 
**Key technologies used:**
- [Microsoft Azure IoT Hub](https://azure.microsoft.com/en-us/services/iot-hub/)
- [Microsoft Azure Stream Analytics](https://azure.microsoft.com/en-us/services/stream-analytics/?v=17.23h)
- [Microsoft Cosmos DB](https://azure.microsoft.com/en-gb/services/cosmos-db/?&wt.mc_id=AID_SEM_7Smac0mv)
- [Microsoft PowerBi](https://powerbi.microsoft.com/en-us/)
 
**Core Team:** 
- Friedrich , Bragi Sensor Development Lead
- Michael Marth, Bragi Mobile Development Lead
- Quintus Stierstorfer, Bragi Mobile Application Architect
- Thorsten , Bragi Telemety Lead
- Pierre , Bragi Insights Team
- [Patric Boscolo, Microsoft DX](https://github.com/patbosc)
- [Oliver Scheer, Microsoft DX](https://github.com/TheOliver)

## Customer profile ##

[Bragi](https://www.bragi.com) is a international company with development based in munich germany. The goal is to innovate the audible computing sector.
Bragi is developing discrete virtual assistants that enable, protect and entertain you. With the launch of The Dash in 2015, Bragi introduced the first smart Hearable to the world. 
**We envision a future where everyone is enabled to achieve their full potential through the pioneering development of deep human machine interaction in hardware and software.**


Bragi most important product today is called "Dash" and it is a true wireless bluethooth earphone. THe Dash can be controlled and managed via a companion app distributed via Apple App Store and Google Play. With several hundred thousand devices shiped Bragi is a key player in the wireless headset market. The dash weights 13g and has a battery life of 3 hours. It has a frequency response of 20 â€“ 20,000Hz with a Bluethooth 4.0 connection. It also ships with features such as tap on the device and an 4gb build in mp3 player. 

 
## Problem statement ##


Bragi is currently using Apache Kafka for telemetry collection and Elastic Search as well as Kibana for analyzing and displaying the complex telemetry data. The infrastructure for kafka and elastic search is hosted on-premise. Reporting and telemetry data is currently taking a lot of on-premise resources and the overall performance and availability of the services could be higher. The arbitrary data format in which Bragi is sending the data makes it hard to generate filters and index based searches as well as using out of hte box analytics. There were several approaches to unify the data model send. As of today there is no structured data format for the messages generated by the devices in their diffrent stages in the Product Lifecycle. Bragi uses Microsoft Teams as main internal communication tool, presenting the data collected and analyzing it in Microsoft Teams would be a plus. Right now there is no way of collecting information form a device which is shipped for support purposes. This would increase the customer satisfaction since remote telemetry can be used to help figure out what is wrong with a device. 

 
## Solution and steps ##

During several meetings with the mobile and sensor development teams we developed a good understanding of the problem set and also identify key success indicators. 

Currently there are several phases or Lifetime Stages of a device. 

During development of a device family there is a test environment where the sensors and the device is connected to a raspberry pi 3. This is stage 0 where the development team and the sensor team want to get detailed information and testing capabilities. 

Once a device family reaches a point where they are marked for testing they are no longer communicating with the raspeberry pi 3 farm. The devices will be connected to a desktop app on PC or Mac. Also during the internal testing phase the device will send less information and will only report key KPIs as well as minimum  sensor information. 

Stage 2 of the devices is the Beta-Test done by Bragi employees and several hand picked testers which will give feedback about the next generation of devices. The companion App from iOS and Android is used to send this information. 

Stage 3 is the production where each device produces is running a check and test to report the overall criterias of the device. 

Stage 4 is the production devices which are bought by a customer and managed via the companion app. 

For this paper we only focus on the Stage 4 devices. We wanted to be able to collect telemetry data from a production device and analyze the data. It was very important to not only collect the telemerty data, but also allowing to send information to the device and also trigger remote methods on the device for the support. We also wanted to be able to show the data in PowerBi since the dashboard could be integrated in Microsoft Teams and Bragi staff can easly access the data without leaving Microsoft Teams.


 ![Architecture Diagram](/images/bragi/architecture.png)
**Technical Diagram from Meeting with the Application Team @ Bragi**

## Technical delivery ##
During our 5 day hackfest with Bragi we build the first implementation. Therefor we talked with the mobile development team and discussed possible scenarious. We also shared the source code with the team via a private repository on github. 

**Security**
- We used https as transport layer and compressed and encrypted the device messages with a basic asymetric encryption. 

**Devices**
- Device used during the diffrent phases of the lifecylce varies. We focused on the iOs and Android Application Companion. 
	
  We focused on the Companion App which is currently running on iOS and Android Smart Phones. So our hardware was Apple iPhone as well as Android Phones.

  We also provided Bragi several samples for implementing the communication for the diffrent clients such as: Raspberry Pi 3, Mac OS, Windows 10 Desktop, Android, iOS.

**Data Ingest**

    We used the latest version of iOS and Android SDKs and the mobile development team already had interfaces for the integration of Microsoft Azure IoT Hub. [Microsoft Azure IoT SDKs for Java](https://github.com/azure/azure-iot-sdk-java)
    In order to get real data from several stages of the product lifecycle to look at and get a better understanding of the ovall data structure we used a connector to [apache kafka sink connnector](https://github.com/Azure/toketi-kafka-connect-iothub/blob/master/README_Sink.md). We had several points in the current architecture so we used another very basic http ingestion of messages with a simple curl call against the system. 

    Send a message via HTTP POST Url Schema:

    https://{IoTHubName}.azure-devices.net/devices/{deviceId}/messages/events?api-version={api-version}

  ```curl
  curl -i -X POST -H "Content-Type: application/json" -H "Authorization: SharedAccessSignature sr={IoTHubName}.azure-devices.net%2Fdevices%2F{devicename}}&sig=[{SharedAccessSignature}]" -d "{deviceId: \"{devicename}\"}" https://{IoTHubName}.azure-devices.net/devices/{devicename}/messages/events?api-version=2016-02-03
  ```

**Ingest**

During the PoC in a 24h period 240,000 messags where send from more than 100+ devices in the development and test environment. The average size of a message was ~52.43 KB and frequency in which messages where send was ~10k messages per hour which is a message every ~36 seconds.  

We wanted to understand the data processing better. So we used historical data provided via logstash. Two Datasets with a failure tolerance and different size where choosen by bragi to do the load and analysis tests for the newly developed system. Logstash data of the development PI Farms show a lot of variety in the data and the size of the logstash log files varies a lot depending on the implemented tests running on the device. We had two diffrent referenced datasets where one had a file size of ~20GB and another having ~13GB.

We developed a loadtest application in c# sending the logstash historical data in a ~10.000 msg/h frequency to iot hub. 

```csharp

/// <summary>
/// Sends the device to cloud messages (async).
/// </summary>
/// <param name="deviceClient">The device client.</param>
/// <param name="ct">The cancellation token</param>
/// <returns></returns>
private static async Task SendDeviceToCloudMessagesAsync(DeviceClient deviceClient, CancellationToken ct)
{
  while (true)
  {
    if (!ct.IsCancellationRequested)
    {
      var messageContent = await GetDataFromLogStashLogAsync();
      var message = new Message(Encoding.ASCII.GetBytes(messageString));
      await deviceClient.SendEventAsync(message);
      Console.WriteLine($"{DateTime.Now} > Sending message: {messageContent}");
      await Task.Delay(30000, ct);
    }
    else
    {
     break;
    }
  }
}

```

The application development team also wanted to test the communication to the test client so we also developed a messages reciever function in the c# test client along with the possiblity to triggering a remote method within the c# client. 

In order to test that method we used the [device explorer](https://github.com/Azure/azure-iot-sdk-csharp/tree/master/tools/DeviceExplorer) which is part of the [Microsoft Azure IoT SDK for .NET](https://github.com/Azure/azure-iot-sdk-csharp). This allowed us to test the messaging from and to the iot hub as well as triggering methods on the client device and also the managing of the test devices. 

```csharp

/// <summary>
/// Receives messages from the cloud send to device.
/// </summary>
/// <param name="deviceClient">The device client from the nuget package.</param>
/// <param name="ct">The CancellationToken.</param>
/// <returns></returns>
private static async Task ReceiveMessageAsync(DeviceClient deviceClient, CancellationToken ct)
{
  Console.WriteLine("Receiving cloud to device messages from service");
  while (true)
  {
    if (ct.IsCancellationRequested) break;
    var receivedMessage = await deviceClient.ReceiveAsync();
    if (receivedMessage == null) continue;

    var messageText = Encoding.ASCII.GetString(receivedMessage.GetBytes());

    Console.ForegroundColor = ConsoleColor.Red;
    Console.WriteLine(messageText);

    // this will remove the message from the queue.  
    await deviceClient.CompleteAsync(receivedMessage);
  }
}

```
*This snippet shows how messages are recieved in a C# Application*

![Screenshot Device Explorer and C# Messaging client](/images/bragi/sendmessagetodevice.png)

Used Reference Implementation and SDKs:

- There is great reference implementation from [William Berry on Github](https://github.com/Microsoft/iot-samples/blob/master/DeviceManagement/csharp/README.md). We used this to compress the messages but it also explains how the device managing is done.
- [Nuget Package for IoT Hub Device Management in C#](https://www.nuget.org/packages/Microsoft.Azure.Devices.Client)
For the Android implementation we used the Java Package for IoT Hub Device Management.
- [Java Package for IoT Hub Device Management in Java](https://mvnrepository.com/artifact/com.microsoft.azure.iothub-java-client)

**Processing the data**

Once we finished the data ingest strategy we looked at the structure of the incomming messages. Given the fact that we had a lot of variety in the data itself we decided to leave the messages as is and store them in a Microsoft Azure Storage Account. That gave us several options for processing the data. 

Since we also wanted to show the advantages of Microsoft Stream Analytic Services we implemented an Microsoft Azure Stream Analytics Job. It basically saves the incoming messages on Microsoft Azure Storage Blob Container.

```TSQL
SELECT
    *
INTO
    blobout
FROM
    iothubin
```

![](/images/bragi/asablob.png)

*Path pattern for daily logs*

![](/images/bragi/blobexp.png)
*Result in Azure Storage Explorer*

Since the Bragi on-premise Elastic Stack was already heavy in use we decided to create our own Elastic Stack in Azure. There is a [Azure Marketplace solution from Bitnami](https://azure.microsoft.com/en-us/blog/bitnami-app-stacks-in-the-azure-marketplace/?v=17.23h). Which makes the installation of the Elastic Stack pretty simple. This way we were able to query refernce data with Kibana from the Logstash data provided by Bragi and also allowed us to compare results. 

**PowerBi**

In our scenario we wanted to look at the Raspberry Pi Test and Development Farm and figure out when a device is on or off. 

![](/images/bragi/kibana.png)
*Kibana view of the raw data field*

We considerd and implemented several diffrent approaches how we can represent the data in fast and meaningfull way. We also wanted to create a great filtering and exploring experience for the users.    

We created a Microsoft Azure Cosmos Database and added it as output to the Stream Analytics Job. 
![](/images/bragi/asa.png) 

```TSQL
SELECT
    *
INTO
    blobout
FROM
    iothubin

SELECT
    *
INTO
    cosmosout
FROM
    iothubin
```
*Final query in Azure Stream Analytics*

The great thing about the Azure Cosmos DB is that without generating an index or processing og the data we were able to create queries against the data.
![](/images/bragi/cosmosdb.png)

```TSQL
SELECT * FROM c WHERE c.message="insertion.v_insertion = confirmed"
```

We decided to user PowerBi Embeded in Microsoft Teams so the diffrent teams would find the information directly in their main communication Tool.


First we created a PowerBi workspace collection on portal.azure.com and Installed the PowerBi CLI Tooling.

```powershell
npm install powerbi-cli -g
```
 
Next we created a PowerBi Workspace with the following command.
 
```powershell
powerbi create-workspace -c <collection> -k <accessKey>
```
 
We created several reports and uploaded the data to our workspace.
 
```powershell
powerbi import -w <workspaceid> -c newsmining -k <accesskey> -f Overview.pbix -n Overview -o
```
 
Last but not least was the integration of the PowerBi Dashboard in Microsoft Teams. Since we were using PowerBi Embeded we added it as a simple Website to the Teams Channel.

![](/images/bragi/bragiteams.jpg) 

The benefit of this implementation is that everyone within the team is able to access the new Dashboard via Microsoft Teams without leaving the main communication system. Users are able to filter and drill down into the presented data. Bragi is also able to define queries and keep the arbitrary Messages. 

Here you can see the full architecture of the Implementation.
![Architecture Overview](/images/bragi/BragiValueStream.png)

## Conclusion ##

Bragi is refering to the solution as "seeing the big picture" since the project is giving several divisions in bragi the chance to build a better product. Since the Product Lifetime and Lifecycle telemetry from the "sensor team" and the data from the "app development team" can be used to build a better customer experience. Another great aspect of this solution is also the fact that the product inovation team can test and measure the customer demands easier in the future. 



